{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# keras import\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import imdb\n",
    "# from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import concatenate, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from datetime import time\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import LSTM, Dropout, Input, Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import he_normal\n",
    "# fix random seed for reproducibility\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(7)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "\n",
       "  clean_categories                 clean_subcategories  \\\n",
       "0     math_science  appliedsciences health_lifescience   \n",
       "1     specialneeds                        specialneeds   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('preprocessed_data.csv')\n",
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(d, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76473, 9), (32775, 9))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data vectorizatio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training word2vec model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_essay_text = train_df.essay.values.tolist()\n",
    "x_test_essay_text = test_df.essay.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49086\n"
     ]
    }
   ],
   "source": [
    "# tokenizing \n",
    "# https://stackoverflow.com/questions/52126539/using-pretrained-gensim-word2vec-embedding-in-keras\n",
    "# https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(x_train_essay_text)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(len(t.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    \n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        \n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 300))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('glove.6B.300d.txt')\n",
    "# raw_embedding = load_embedding('glove.42B.300d.txt')\n",
    "\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, t.word_index)\n",
    "where_are_NaNs = np.isnan(embedding_vectors)\n",
    "embedding_vectors[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.any(np.isnan(embedding_vectors))\n",
    "# np.all(np.isfinite(embedding_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own Word2Vec model using your own text corpus\n",
    "# def sentenceSplit(data):\n",
    "#     list_of_sentance=[]\n",
    "#     for sentance in data:\n",
    "#         list_of_sentance.append(sentance.split())\n",
    "#     return list_of_sentance\n",
    "\n",
    "# # this line of code trains your w2v model on the give list of sentances\n",
    "# list_of_sentance_train = sentenceSplit(x_train_essay_text)\n",
    "# w2v_model=Word2Vec(list_of_sentance_train, size=150, workers=4)\n",
    "# w2v_words = list(w2v_model.wv.vocab)\n",
    "\n",
    "# w2v_model.save(\"dc_lstm_w2vmodel\")\n",
    "# Load pre-trained Word2Vec model.\n",
    "\n",
    "# w2v_model = Word2Vec.load(\"dc_lstm_w2vmodel\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Train and Test data for text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76473, 328)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "# enocde it to sequences\n",
    "encoded_docs = t.texts_to_sequences(x_train_essay_text)\n",
    "\n",
    "# padding\n",
    "max_length = len(max(x_train_essay_text, key=len).split(' '))\n",
    "\n",
    "x_train_text = sequence.pad_sequences(encoded_docs, maxlen = max_length, padding='post')\n",
    "x_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32775, 328)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "# enocde it to sequences\n",
    "encoded_docs = t.texts_to_sequences(x_test_essay_text)\n",
    "x_test_text = sequence.pad_sequences(encoded_docs, maxlen = max_length, padding='post')\n",
    "x_test_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a weight matrix for the Embedding layer from a loaded embedding\n",
    "# def get_weight_matrix(embedding, vocab):\n",
    "#     # total vocabulary size plus 0 for unknown words\n",
    "#     vocab_size = len(vocab) + 1\n",
    "#     # define weight matrix dimensions with all 0\n",
    "#     weight_matrix = np.zeros((vocab_size, w2v_model.vector_size))\n",
    "    \n",
    "#     # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "#     for word, i in vocab.items():\n",
    "#         if word not in w2v_model.wv.vocab:\n",
    "#             continue\n",
    "            \n",
    "#         weight_matrix[i] = embedding.wv[word]# embedding.get(word)\n",
    "        \n",
    "#     return weight_matrix\n",
    "\n",
    "# # load embedding from file\n",
    "# # raw_embedding = load_embedding('glove.42B.300d.txt')\n",
    "\n",
    "# # get vectors in the right order\n",
    "# # embedding_vectors = get_weight_matrix(raw_embedding, t.word_index)\n",
    "# embedding_vectors = get_weight_matrix(w2v_model, t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_vectors[49080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# e = Embedding(vocab_size, 128, weights=[embedding_vectors], input_length=x_train_text.shape[1], trainable=False)\n",
    "# model.add(e)\n",
    "# model.add(Flatten())\n",
    "\n",
    "# # summarize the model\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 school state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_df.school_state.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sch_state = train_df.school_state.values.tolist()\n",
    "test_sch_state = test_df.school_state.values.tolist()\n",
    "# no_words = len(set(train_sch_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sch_state)\n",
    "train_sch_state = tokenizer.texts_to_sequences(train_sch_state)\n",
    "test_sch_state = tokenizer.texts_to_sequences(test_sch_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76473, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "max_length = len(max(train_sch_state, key=len))\n",
    "x_train_sch_state = sequence.pad_sequences(train_sch_state, maxlen = max_length, padding='post')\n",
    "x_train_sch_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32775, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "x_test_sch_state = sequence.pad_sequences(test_sch_state, maxlen = max_length, padding='post')\n",
    "x_test_sch_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Project grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_df.project_grade_category.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['project_grade_category'] = train_df['project_grade_category'].str.replace('_','')\n",
    "test_df['project_grade_category'] = test_df['project_grade_category'].str.replace('_','')\n",
    "# max_length = 1 #len(max(train_proj_grade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proj_grade = train_df.project_grade_category.values.tolist()\n",
    "test_proj_grade = test_df.project_grade_category.values.tolist()\n",
    "max_length = 1 #len(max(train_proj_grade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_proj_grade)\n",
    "train_proj_grade = tokenizer.texts_to_sequences(train_proj_grade)\n",
    "test_proj_grade = tokenizer.texts_to_sequences(test_proj_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradesprek2': 1, 'grades35': 2, 'grades68': 3, 'grades912': 4}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76473, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "x_train_proj_grade = sequence.pad_sequences(train_proj_grade, maxlen = max_length, padding='post')\n",
    "x_train_proj_grade.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32775, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "x_test_proj_grade = sequence.pad_sequences(test_proj_grade, maxlen = max_length, padding='post')\n",
    "x_test_proj_grade.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 clean_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_df.clean_categories.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_cat = train_df.clean_categories.values.tolist()\n",
    "test_clean_cat = test_df.clean_categories.values.tolist()\n",
    "# max_length = len(max(train_clean_cat, key=len).split(' '))\n",
    "max_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_clean_cat)\n",
    "train_clean_cat = tokenizer.texts_to_sequences(train_clean_cat)\n",
    "test_clean_cat = tokenizer.texts_to_sequences(test_clean_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76473, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "x_train_clean_cat = sequence.pad_sequences(train_clean_cat, maxlen = max_length, padding='post')\n",
    "x_train_clean_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32775, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "x_test_clean_cat = sequence.pad_sequences(test_clean_cat, maxlen = max_length, padding='post')\n",
    "x_test_clean_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 clean_sub_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_df.clean_subcategories.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.clean_subcategories.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_sub_cat = train_df.clean_subcategories.values.tolist()\n",
    "test_clean_sub_cat = test_df.clean_subcategories.values.tolist()\n",
    "max_length = 1#len(max(train_clean_sub_cat, key=len).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_clean_sub_cat)\n",
    "train_clean_sub_cat = tokenizer.texts_to_sequences(train_clean_sub_cat)\n",
    "test_clean_sub_cat = tokenizer.texts_to_sequences(test_clean_sub_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76473, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "x_train_clean_sub_cat = sequence.pad_sequences(train_clean_sub_cat, maxlen = max_length, padding='post')\n",
    "x_train_clean_sub_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32775, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "x_test_clean_sub_cat = sequence.pad_sequences(test_clean_sub_cat, maxlen = max_length, padding='post')\n",
    "x_test_clean_sub_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 Teacher_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_df.teacher_prefix.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_teacher_prefix = train_df.teacher_prefix.values.tolist()\n",
    "test_teacher_prefix = test_df.teacher_prefix.values.tolist()\n",
    "max_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_teacher_prefix)\n",
    "train_teacher_prefix = tokenizer.texts_to_sequences(train_teacher_prefix)\n",
    "test_teacher_prefix = tokenizer.texts_to_sequences(test_teacher_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76473, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "x_train_teacher_prefix = sequence.pad_sequences(train_teacher_prefix, maxlen = max_length, padding='post')\n",
    "x_train_teacher_prefix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32775, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train data\n",
    "x_test_teacher_prefix = sequence.pad_sequences(test_teacher_prefix, maxlen = max_length, padding='post')\n",
    "x_test_teacher_prefix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of previously posted project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_previously_posted_projects = train_df['teacher_number_of_previously_posted_projects']#train_df.teacher_number_of_previously_posted_projects.values\n",
    "#x_train_previously_posted_projects = x_train_previously_posted_projects.reshape(76473, 1)\n",
    "#x_train_previously_posted_projects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_previously_posted_projects = test_df['teacher_number_of_previously_posted_projects']#test_df.teacher_number_of_previously_posted_projects.values\n",
    "#x_test_previously_posted_projects = x_test_previously_posted_projects.reshape(32775, 1)\n",
    "#x_test_previously_posted_projects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_previously_posted_projects.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 1\n",
    "#### Model-1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FullSizeImage](https://i.imgur.com/w395Yk9.png)\n",
    "<img src='https://i.imgur.com/w395Yk9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.x = Dropout(0.5)(x) output = Dense(2, activation = 'softmax')(x)\n",
    "\n",
    "# Can you try removing this dropout and see if the performance could be imporved?\n",
    "\n",
    "# 2. For model 2 , take idf range from 2 to 10 for keeping the words. This would increase the auc score. \n",
    "\n",
    "# 3. Make sure the test AUC for all the three models is more than 0.70. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "def auroc(y_true, y_pred):\n",
    "    # print(y_true, y_pred)\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input 1\n",
    "input_1 = Input(shape=(328,))\n",
    "x1 =Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=x_train_text.shape[1], trainable=False)(input_1)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "x1 = LSTM(128,return_sequences=True)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "#input 2\n",
    "input_2 = Input(shape=(1,))\n",
    "x2 = Embedding(input_dim= 52, output_dim= 2)(input_2)\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "#input 3\n",
    "input_3 = Input(shape=(1,))\n",
    "x3 = Embedding(input_dim = 5, output_dim= 2)(input_3)\n",
    "x3 = Flatten()(x3)\n",
    "\n",
    "#input 4\n",
    "input_4 = Input(shape=(1,))\n",
    "x4 = Embedding(input_dim=52,output_dim= 2)(input_4)\n",
    "x4 = Flatten()(x4)\n",
    "\n",
    "#input 5\n",
    "input_5 = Input(shape=(1,))\n",
    "x5 = Embedding(input_dim= 396, output_dim= 64)(input_5)\n",
    "x5 = Flatten()(x5)\n",
    "\n",
    "#input 6\n",
    "input_6 = Input(shape=(1,))\n",
    "x6 = Embedding(input_dim= 6,output_dim= 4)(input_6)\n",
    "x6 = Flatten()(x6)\n",
    "\n",
    "#input 7\n",
    "input_7 = Input(shape=(1,))\n",
    "x7 = Dense(16,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(input_7)\n",
    "\n",
    "#merging all the inputs \n",
    "concat = concatenate([x1,x2,x3,x4,x5,x6,x7])\n",
    "#x = BatchNormalization()(concat)\n",
    "\n",
    "x = Dense(128, activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(concat)\n",
    "x = Dense(64,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(2, activation = 'softmax')(x)\n",
    "\n",
    "# model with all the inputs\n",
    "model = Model([input_1, input_2, input_3, input_4, input_5, input_6, input_7], output)\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0006,decay = 1e-4), metrics=[auroc])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.project_is_approved.values.tolist()\n",
    "y_test = test_df.project_is_approved.values.tolist()\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train =  [x_train_text, x_train_sch_state, x_train_proj_grade, x_train_clean_cat, x_train_clean_sub_cat, x_train_teacher_prefix, x_train_previously_posted_projects]\n",
    "x_test = [x_test_text, x_test_sch_state, x_test_proj_grade, x_test_clean_cat, x_test_clean_sub_cat, x_test_teacher_prefix, x_test_previously_posted_projects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,tensorboard]\n",
    "model.fit(x_train, y_train, nb_epoch=20,verbose=1,batch_size=128, callbacks =callbacks_list)\n",
    "# model.save('dc_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-39-381ad9f4dd6c>:4: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /home/manish/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/keras-team/keras/issues/10104\n",
    "dependencies = {\n",
    "    'auroc': auroc\n",
    "}\n",
    "model = load_model(\"dc_model2.h5\", custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.9791035547455924\n"
     ]
    }
   ],
   "source": [
    "### Testing model-1\n",
    "y_train_pred = model.predict(x_train)\n",
    "print(\"Train AUC:\",roc_auc_score(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7008411718577311\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(x_test)\n",
    "print(\"Test AUC:\",roc_auc_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the TF-IDF on the Train data.\n",
    "- Get the idf value for each word we have in the train data.\n",
    "- Remove the low idf value and high idf value words from our data. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very frequent words and very very rare words don't give much information. (you can plot a box plots and take only the idf scores within IQR range and corresponding words)\n",
    "- Train the LSTM after removing the Low and High idf value words. (In model-1 Train on total data but in Model-2 train on data after removing some words based on IDF values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "def saveModel(filename, model):\n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "\n",
    "    except ImportError:\n",
    "        import pickle\n",
    "\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(model, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "def getModel(pkl_filename):\n",
    "    with open(pkl_filename, 'rb') as file:\n",
    "        pickle_model = pickle.load(file)\n",
    "    return pickle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_essay_text = train_df.essay.values.tolist()\n",
    "x_test_essay_text = test_df.essay.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10, max_features=10000)\n",
    "vectorizer.fit(x_train_essay_text) \n",
    "x_train_tfidf  = vectorizer.transform(x_train_essay_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'IDF Value')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot to decide the threshold\n",
    "plt.boxplot(vectorizer.idf_)\n",
    "plt.ylabel(\"IDF Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Percentile: 1.0077317773707817\n",
      "10 Percentile: 4.961189206264439\n",
      "20 Percentile: 5.876176147118889\n",
      "30 Percentile: 6.59925919512169\n",
      "40 Percentile: 7.108907655714666\n",
      "50 Percentile: 7.5442257269725115\n",
      "60 Percentile: 7.887997266075336\n",
      "70 Percentile: 8.201654824930378\n",
      "80 Percentile: 8.50703647448156\n",
      "90 Percentile: 8.7789701899652\n",
      "100 Percentile: 9.846810819966556\n"
     ]
    }
   ],
   "source": [
    "# sortedDiff =np.sort(diff)\n",
    "for i in range (0,101,10):\n",
    "    p = np.percentile(vectorizer.idf_, i)\n",
    "    print(str(i)+\" Percentile: \"+ str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After 70th percentile idf value nearly remain same. </p>\n",
    "    -min_threshold = 20th percentile<br>\n",
    "    -max_threshold = 80th percentile<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 8.7789701899652\n"
     ]
    }
   ],
   "source": [
    "min_threshold = 3 # np.percentile(vectorizer.idf_, 20)\n",
    "max_threshold = np.percentile(vectorizer.idf_, 90)\n",
    "print(min_threshold, max_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = vectorizer.get_feature_names()\n",
    "idf_val = vectorizer.idf_\n",
    "len(feat), len(idf_val)\n",
    "feat_idf_dict = dict(zip(feat, idf_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8831"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_data = {k: v for k, v in feat_idf_dict.iteritems() if min_threshold < v[0] < max_threshold}\n",
    "for k  in list(feat_idf_dict.keys()):\n",
    "    # removing low and high idf features\n",
    "    if (min_threshold >= feat_idf_dict[k]) or (feat_idf_dict[k] >= max_threshold):\n",
    "        feat_idf_dict.pop(k)\n",
    "\n",
    "len(feat_idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing low and high idf words from dataset\n",
    "def get_filtered_text(text_dataset):\n",
    "    \n",
    "    filtered_text = []\n",
    "    for text in tqdm(text_dataset):\n",
    "        resultwords  = [word for word in text.split() if word.lower() in list(feat_idf_dict.keys())]\n",
    "        result = ' '.join(resultwords)\n",
    "        filtered_text.append(result)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_essay_text_filtered = get_filtered_text(x_train_essay_text)\n",
    "# saveModel(\"x_train_tfidf_filter_3.pkl\", x_train_essay_text_filtered)\n",
    "\n",
    "# x_test_essay_text_filtered = get_filtered_text(x_test_essay_text)\n",
    "# saveModel(\"x_test_tfidf_filter_3.pkl\", x_test_essay_text_filtered)\n",
    "\n",
    "x_train_essay_text_filtered = getModel(\"x_train_tfidf_filter_3.pkl\")\n",
    "x_test_essay_text_filtered = getModel(\"x_test_tfidf_filter_3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8831\n"
     ]
    }
   ],
   "source": [
    "# tokenizing \n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(x_train_essay_text_filtered)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(len(t.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    \n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        \n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 200))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('glove.6B/glove.6B.200d.txt')\n",
    "# raw_embedding = load_embedding('glove.42B.300d.txt')\n",
    "\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, t.word_index)\n",
    "where_are_NaNs = np.isnan(embedding_vectors)\n",
    "embedding_vectors[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32775, 216)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "# enocde it to sequences\n",
    "encoded_docs = t.texts_to_sequences(x_train_essay_text_filtered)\n",
    "max_length = len(max(x_train_essay_text_filtered, key=len).split(' '))\n",
    "x_train_text_M2 = sequence.pad_sequences(encoded_docs, maxlen = max_length, padding='post')\n",
    "x_train_text_M2.shape\n",
    "\n",
    "# Test\n",
    "encoded_docs = t.texts_to_sequences(x_test_essay_text_filtered)\n",
    "x_test_text_M2 = sequence.pad_sequences(encoded_docs, maxlen = max_length, padding='post')\n",
    "x_test_text_M2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76473, 216)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text_M2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "def auroc(y_true, y_pred):\n",
    "    # print(y_true, y_pred)\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-381ad9f4dd6c>:4: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 216)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 216, 200)     1766400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 216, 200)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 216, 128)     168448      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 2)         104         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 2)         10          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 2)         104         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 64)        25344       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 4)         24          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 27648)        0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 64)           0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4)            0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           32          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 27738)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          3550592     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           256         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           2080        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            66          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,521,716\n",
      "Trainable params: 3,755,188\n",
      "Non-trainable params: 1,766,528\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#input 1\n",
    "input_1 = Input(shape=(216,))\n",
    "x1 =Embedding(vocab_size, 200, weights=[embedding_vectors], input_length=x_train_text_M2.shape[1], trainable=False)(input_1)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "x1 = LSTM(128,return_sequences=True)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "#input 2\n",
    "input_2 = Input(shape=(1,))\n",
    "x2 = Embedding(input_dim= 52, output_dim= 2)(input_2)\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "#input 3\n",
    "input_3 = Input(shape=(1,))\n",
    "x3 = Embedding(input_dim = 5, output_dim= 2)(input_3)\n",
    "x3 = Flatten()(x3)\n",
    "\n",
    "#input 4\n",
    "input_4 = Input(shape=(1,))\n",
    "x4 = Embedding(input_dim=52,output_dim= 2)(input_4)\n",
    "x4 = Flatten()(x4)\n",
    "\n",
    "#input 5\n",
    "input_5 = Input(shape=(1,))\n",
    "x5 = Embedding(input_dim= 396, output_dim= 64)(input_5)\n",
    "x5 = Flatten()(x5)\n",
    "\n",
    "#input 6\n",
    "input_6 = Input(shape=(1,))\n",
    "x6 = Embedding(input_dim= 6,output_dim= 4)(input_6)\n",
    "x6 = Flatten()(x6)\n",
    "\n",
    "#input 7\n",
    "input_7 = Input(shape=(1,))\n",
    "x7 = Dense(16,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(input_7)\n",
    "\n",
    "#merging all the inputs \n",
    "concat = concatenate([x1,x2,x3,x4,x5,x6,x7])\n",
    "#x = BatchNormalization()(concat)\n",
    "\n",
    "x = Dense(128, activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(concat)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(2, activation = 'softmax')(x)\n",
    "\n",
    "# model with all the inputs\n",
    "model = Model([input_1, input_2, input_3, input_4, input_5, input_6, input_7], output)\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0006,decay = 1e-4), metrics=[auroc])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = train_df.project_is_approved.values.tolist()\n",
    "y_test = test_df.project_is_approved.values.tolist()\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =  [x_train_text_M2, x_train_sch_state, x_train_proj_grade, x_train_clean_cat, x_train_clean_sub_cat, x_train_teacher_prefix, x_train_previously_posted_projects]\n",
    "x_test = [x_test_text_M2, x_test_sch_state, x_test_proj_grade, x_test_clean_cat, x_test_clean_sub_cat, x_test_teacher_prefix, x_test_previously_posted_projects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/manish/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manish/.local/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manish/.local/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/20\n",
      "76473/76473 [==============================] - 232s 3ms/step - loss: 0.6362 - auroc: 0.5400\n",
      "WARNING:tensorflow:From /home/manish/.local/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/20\n",
      "76473/76473 [==============================] - 245s 3ms/step - loss: 0.4998 - auroc: 0.5909\n",
      "Epoch 3/20\n",
      "76473/76473 [==============================] - 255s 3ms/step - loss: 0.4864 - auroc: 0.5318\n",
      "Epoch 4/20\n",
      "76473/76473 [==============================] - 297s 4ms/step - loss: 0.4682 - auroc: 0.5128\n",
      "Epoch 5/20\n",
      "76473/76473 [==============================] - 346s 5ms/step - loss: 0.4554 - auroc: 0.5356\n",
      "Epoch 6/20\n",
      "76473/76473 [==============================] - 303s 4ms/step - loss: 0.4431 - auroc: 0.5962\n",
      "Epoch 7/20\n",
      "76473/76473 [==============================] - 312s 4ms/step - loss: 0.4369 - auroc: 0.5950\n",
      "Epoch 8/20\n",
      "76473/76473 [==============================] - 307s 4ms/step - loss: 0.4259 - auroc: 0.6503\n",
      "Epoch 9/20\n",
      "76473/76473 [==============================] - 285s 4ms/step - loss: 0.4156 - auroc: 0.6784\n",
      "Epoch 10/20\n",
      "76473/76473 [==============================] - 262s 3ms/step - loss: 0.4130 - auroc: 0.6895\n",
      "Epoch 11/20\n",
      "76473/76473 [==============================] - 336s 4ms/step - loss: 0.4097 - auroc: 0.6977\n",
      "Epoch 12/20\n",
      "76473/76473 [==============================] - 307s 4ms/step - loss: 0.4064 - auroc: 0.7036\n",
      "Epoch 13/20\n",
      "76473/76473 [==============================] - 318s 4ms/step - loss: 0.4034 - auroc: 0.7109\n",
      "Epoch 14/20\n",
      "76473/76473 [==============================] - 301s 4ms/step - loss: 0.4019 - auroc: 0.7146\n",
      "Epoch 15/20\n",
      "76473/76473 [==============================] - 307s 4ms/step - loss: 0.3993 - auroc: 0.7207\n",
      "Epoch 16/20\n",
      "76473/76473 [==============================] - 282s 4ms/step - loss: 0.3979 - auroc: 0.7229\n",
      "Epoch 17/20\n",
      "76473/76473 [==============================] - 291s 4ms/step - loss: 0.3961 - auroc: 0.7311\n",
      "Epoch 18/20\n",
      "76473/76473 [==============================] - 270s 4ms/step - loss: 0.3928 - auroc: 0.7361\n",
      "Epoch 19/20\n",
      "76473/76473 [==============================] - 271s 4ms/step - loss: 0.3913 - auroc: 0.7412\n",
      "Epoch 20/20\n",
      "76473/76473 [==============================] - 272s 4ms/step - loss: 0.3885 - auroc: 0.7484\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "filepath = \"weight_model2_v3.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,tensorboard]\n",
    "model.fit(x_train, y_train, nb_epoch=20,verbose=1,batch_size=128, callbacks =callbacks_list)\n",
    "model.save('dc_model2_v4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/issues/10104\n",
    "dependencies = {\n",
    "    'auroc': auroc\n",
    "}\n",
    "\n",
    "model = load_model(\"dc_model2_v3.h5\", custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.7860399617088681\n"
     ]
    }
   ],
   "source": [
    "### Testing model-2\n",
    "y_train_pred = model.predict(x_train)\n",
    "print(\"Train AUC:\",roc_auc_score(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7298947683956255\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(x_test)\n",
    "print(\"Test AUC:\",roc_auc_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FullSizeImage](https://i.imgur.com/fkQ8nGo.png)\n",
    "<img src='https://i.imgur.com/fkQ8nGo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- __input_seq_total_text_data__: <br>\n",
    "<pre>\n",
    "    . Use text column('essay'), and use the Embedding layer to get word vectors. <br>\n",
    "    . Use given predefined glove word vectors, don't train any word vectors. <br>\n",
    "    . Use LSTM that is given above, get the LSTM output and Flatten that output. <br>\n",
    "    . You are free to preprocess the input text as you needed. <br>\n",
    "</pre>\n",
    "- __Other_than_text_data__:<br>\n",
    "<pre>\n",
    "    . Convert all your Categorical values to onehot coded and then concatenate all these onehot vectors <br>\n",
    "    . Neumerical values and use <a href='https://keras.io/getting-started/sequential-model-guide/#sequence-classification-with-1d-convolutions'>CNN1D</a> as shown in above figure. <br>\n",
    "    . You are free to choose all CNN parameters like kernel sizes, stride.<br>\n",
    "    \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76473, 51) (32775, 51)\n"
     ]
    }
   ],
   "source": [
    "# http://flovv.github.io/Embeddings_with_keras_part2/\n",
    "# school state\n",
    "token = CountVectorizer()\n",
    "# integer encode the documents\n",
    "x_train_sch_state = token.fit_transform(train_df.school_state)\n",
    "x_test_sch_state = token.transform(test_df.school_state)\n",
    "print(x_train_sch_state.shape, x_test_sch_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x_train_sch_state, x_train_proj_grade, x_train_clean_cat, x_train_clean_sub_cat, x_train_teacher_prefix, x_train_previously_posted_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76473, 4) (32775, 4)\n"
     ]
    }
   ],
   "source": [
    "# proj_grade\n",
    "token = CountVectorizer()\n",
    "# integer encode the documents\n",
    "x_train_proj_grade = token.fit_transform(train_df.project_grade_category)\n",
    "x_test_proj_grade = token.transform(test_df.project_grade_category)\n",
    "print(x_train_proj_grade.shape, x_test_proj_grade.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76473, 9) (32775, 9)\n"
     ]
    }
   ],
   "source": [
    "# clean_cat\n",
    "token = CountVectorizer()\n",
    "# integer encode the documents\n",
    "x_train_clean_cat = token.fit_transform(train_df.clean_categories)\n",
    "x_test_clean_cat = token.transform(test_df.clean_categories)\n",
    "print(x_train_clean_cat.shape, x_test_clean_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76473, 30) (32775, 30)\n"
     ]
    }
   ],
   "source": [
    "# x_train_clean_sub_cat\n",
    "token = CountVectorizer()\n",
    "# integer encode the documents\n",
    "x_train_clean_sub_cat = token.fit_transform(train_df.clean_subcategories)\n",
    "x_test_clean_sub_cat = token.transform(test_df.clean_subcategories)\n",
    "print(x_train_clean_sub_cat.shape, x_test_clean_sub_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76473, 5) (32775, 5)\n"
     ]
    }
   ],
   "source": [
    "# x_train_teacher_prefix\n",
    "token = CountVectorizer()\n",
    "# integer encode the documents\n",
    "x_train_teacher_prefix = token.fit_transform(train_df.teacher_prefix)\n",
    "x_test_teacher_prefix = token.transform(test_df.teacher_prefix)\n",
    "print(x_train_teacher_prefix.shape, x_test_teacher_prefix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76473, 1) (32775, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_previously_posted_projects = train_df.teacher_number_of_previously_posted_projects.values\n",
    "x_train_previously_posted_projects = x_train_previously_posted_projects.reshape(76473, 1)\n",
    "#x_train_previously_posted_projects.shape\n",
    "x_test_previously_posted_projects = test_df.teacher_number_of_previously_posted_projects.values\n",
    "x_test_previously_posted_projects = x_test_previously_posted_projects.reshape(32775, 1)\n",
    "print(x_train_previously_posted_projects.shape, x_test_previously_posted_projects.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input_1\n",
    "x_train_1 =  x_train_text\n",
    "x_test_1 =  x_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76473, 100, 1), (32775, 100, 1))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### input 2\n",
    "x_train_2 = hstack((x_train_sch_state, x_train_proj_grade, x_train_clean_cat, x_train_clean_sub_cat, x_train_teacher_prefix, x_train_previously_posted_projects)).todense()\n",
    "x_train_2 = np.array(x_train_2).reshape(x_train_2.shape[0],x_train_2.shape[1],1)\n",
    "\n",
    "x_test_2 = hstack((x_test_sch_state, x_test_proj_grade, x_test_clean_cat, x_test_clean_sub_cat, x_test_teacher_prefix, x_test_previously_posted_projects)).todense()\n",
    "x_test_2 = np.array(x_test_2).reshape(x_test_2.shape[0],x_test_2.shape[1],1)\n",
    "\n",
    "x_train_2.shape, x_test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "#     print(y_true, y_pred)\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_65_target:0\", shape=(?, ?), dtype=float32) Tensor(\"dense_65/Softmax:0\", shape=(?, 2), dtype=float32)\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_40 (InputLayer)           (None, 328)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 328, 300)     14726100    input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_41 (InputLayer)           (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 328, 300)     0           embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 98, 128)      512         input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 328, 128)     219648      dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 96, 64)       24640       conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_36 (Flatten)            (None, 41984)        0           lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_37 (Flatten)            (None, 6144)         0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 48128)        0           flatten_36[0][0]                 \n",
      "                                                                 flatten_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 128)          6160512     concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 128)          0           dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 64)           8256        dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 64)           0           dense_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64)           256         dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 32)           2080        batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 32)           0           dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 2)            66          dropout_65[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 21,142,070\n",
      "Trainable params: 6,415,842\n",
      "Non-trainable params: 14,726,228\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# input 1\n",
    "input_1 = Input(shape=(328,))\n",
    "x1 =Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=x_train_text.shape[1], trainable=False)(input_1)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "x1 = LSTM(128,return_sequences=True)(x1)\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "# input 2\n",
    "input_2 = Input(shape=(100,1))\n",
    "x2 = Conv1D(filters=128,kernel_size=3, strides=1)(input_2)\n",
    "x2 = Conv1D(filters=64,kernel_size=3, strides=1)(x2)\n",
    "x2 = Flatten()(x2)\n",
    "\n",
    "\n",
    "# merging both the inputs\n",
    "concat = concatenate([x1,x2])\n",
    "x = Dense(128,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(concat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32,activation='relu',kernel_initializer=he_normal(),kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dropout(0.6)(x)\n",
    "output = Dense(2, activation = 'softmax')(x)\n",
    " \n",
    "# create model with two inputs\n",
    "model = Model([input_1,input_2], output)\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0006,decay = 1e-4), metrics=[auroc])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = train_df.project_is_approved.values.tolist()\n",
    "y_test = test_df.project_is_approved.values.tolist()\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x_train_1, x_train_2]\n",
    "x_test  = [x_test_1, x_test_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint,tensorboard]\n",
    "model.fit(x_train, y_train, nb_epoch=20,verbose=1,batch_size=128, callbacks =callbacks_list)\n",
    "# model.save('dc_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/keras-team/keras/issues/10104\n",
    "dependencies = {\n",
    "    'auroc': auroc\n",
    "}\n",
    "\n",
    "model = load_model(\"dc_model3.h5\", custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.7629204231818827\n",
      "Test AUC: 0.7440116073673231\n"
     ]
    }
   ],
   "source": [
    "### Testing model-3\n",
    "y_train_pred = model.predict(x_train)\n",
    "print(\"Train AUC:\",roc_auc_score(y_train,y_train_pred))\n",
    "\n",
    "y_test_pred = model.predict(x_test)\n",
    "print(\"Test AUC:\",roc_auc_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+\n",
      "| Architecture | Train AUC | Test AUC |\n",
      "+--------------+-----------+----------+\n",
      "|   Model-1    |   0.979   |  0.701   |\n",
      "|   Model-2    |   0.786   |  0.728   |\n",
      "|   Model-3    |   0.762   |  0.744   |\n",
      "+--------------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable    \n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Architecture\", \"Train AUC\", \"Test AUC\"]\n",
    "x.add_row([\"Model-1\", \"0.979\", \"0.701\"])\n",
    "x.add_row([\"Model-2\", \"0.786\", \"0.728\"])\n",
    "x.add_row([\"Model-3\", \"0.762\", \"0.744\"])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Heavy overfitting can be seen in model-1. \n",
    " - Model-2 is less overfitted with good accuracy.\n",
    " - Model-3 performs better than model-1 and model-2 with less overfitting and good auc score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
